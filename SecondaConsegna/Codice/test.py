import sys
import copy
import json
from string import atoi
from datetime import datetime
from pyspark import SparkContext, SparkConf
from collections import defaultdict
from itertools import imap, combinations



def create_candidates(itemSet, length):
        return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])


# Checks occurrences of items in transactions and returns frequent items
# input:
#    items: a set of frequent items (candidates)
#     transactions: list of sets representing baskets
#  returns:
#      _itemSet: a set containing all frequent candidates (a subset of inputs)

def frequent_items(items, transactions, min_sup):
    _itemSet = set()
    counter = defaultdict(int)
    localDict = defaultdict(int)
    for item in items:
        for transaction in transactions:
            if item.issubset(transaction):localDict[item] += 1

    for item, count in localDict.items():
        if count >= min_sup:
            _itemSet.add(item)
    return _itemSet


def get_frequent_items_sets(data,min_sup,steps=0):

    # we transform the dataset in a list of sets
    transactions = list()

    # Temporary dictionary to count occurrences
    items = defaultdict(lambda: 0)

    # Returned dictionary
    solution = dict()
    L_set = set()

    # Fills transactions and counts "singletons"
    for line in data:
        transaction = set(line)
        transactions.append(transaction)
        for element in transaction:
            items[element]+=1

    # Add to the solution all frequent items
    for item, count in items.iteritems():
        if count >= min_sup:
            L_set.add(frozenset([item]))

    # Generalize the steps it ends when there are no candidates
    # or if the user provided as input the number of parameters
    k = 2
    solution[k-1] = L_set
    while L_set != set([]) and k != steps+1:
        L_set = create_candidates(L_set,k)
        C_set = frequent_items(L_set,transactions,min_sup)
        if C_set != set([]): solution[k]=C_set
        L_set = C_set
        k = k + 1
    return solution



def findFrequentItemsets(input, s, sc):
    """
    Find frequent item sets using the SON algorithm in two stages.
    First stage: divide document and find frequent itemsets in each partition.
    Second stage: join local itemset candidates, distribute to workers and
    count actual frequency.
    Args:
        arg1 (string): Location of the data file
        arg2 (string): Where to save itemsets
        arg3 (int): Number of partitions to make. Leave empty for default
        arg4 (float): Threshold
        arg5 (SparkContext): Spark Context
    Returns:
        list: List of all the encountered frequent itemsets. There is no
        guarantee that all frequent itemsets were found. But if something is
        in the list, it must be a frequent itemset.
    """

    data = sc.textFile(input)

    numPartitions = data.getNumPartitions()

    count = data.count()

    threshold = s*count

    #split string baskets into lists of items
    baskets = data.map(lambda line: sorted([y for y in line.strip().split(' ')]))

    #treat a basket as a set for fast check if candidate belongs
    basketSets = baskets.map(set).persist()

    #each worker calculates the itemsets of his partition
    localItemSets = baskets.mapPartitions(lambda data: [x for y in get_frequent_items_sets(data, threshold/numPartitions).values() for x in y], True)
    #for reducing by key later
    allItemSets = localItemSets.map(lambda n_itemset: (n_itemset,1))

    #merge candidates that are equal, but generated by different workers
    mergedCandidates = allItemSets.reduceByKey(lambda x,y: x).map(lambda (x,y): x)

    #distribute global candidates to all workers
    mergedCandidates = mergedCandidates.collect()
    candidates = sc.broadcast(mergedCandidates)

    #count actual occurrence of candidates in document
    counts = basketSets.flatMap(lambda line: [(candidate,1) for candidate in candidates.value if line.issuperset(candidate)])

    #filter finalists
    finalItemSets = counts.reduceByKey(lambda v1, v2: v1+v2).filter(lambda (i,v): v>=threshold)

    #put into nice format
    #finalItemSets = finalItemSets.map(lambda (itemset, count): ", ".join([str(x) for x in itemset])+"\t("+str(count)+")")

    #finalItemSets.saveAsTextFile(output)
    return localItemSets



if __name__ == "__main__":

    APP_NAME = "SON-apriori"

    conf = SparkConf().setAppName(APP_NAME)
    conf = conf.setMaster("local[*]")

    sc  = SparkContext(conf=conf)

    f_input = "/Users/francescobenetello/Documents/Dataset/sample.txt"
    threshold = 0.001
    '''
    if len(sys.argv) > 4:
        numPartitions = int(sys.argv[4])
    else:
        numPartitions = None
    '''
    START_TIME = datetime.now()
    ciao = findFrequentItemsets(f_input, threshold, sc)
    END_TIME = datetime.now()
    TIME = format(END_TIME-START_TIME)
    print("Tempo di esecuzione: " + str(TIME) + " sec")    
    for x in ciao.collect():
        print x